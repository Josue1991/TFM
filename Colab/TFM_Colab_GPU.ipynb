{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6460f56",
   "metadata": {},
   "source": [
    "# üéì TFM - CNN + LSTM Training with GPU\n",
    "## Fase 1: CNN (Fashion MNIST + CIFAR-10)\n",
    "## Fase 2: LSTM (ECG5000 + UCI HAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ad80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup inicial\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "# Verificar GPU\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(f\"GPU available: {len(gpus)} - {gpus}\")\n",
    "print(f\"TensorFlow: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b723150b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar dependencias\n",
    "!pip install -q pandas scikit-learn matplotlib\n",
    "print(\"‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c83b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir modelo LSTM\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def build_lstm_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Bidirectional(layers.LSTM(64, return_sequences=True)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Bidirectional(layers.LSTM(32)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print(\"‚úÖ Model function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f223727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar ECG5000\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING ECG5000\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "np.random.seed(42)\n",
    "n_samples = 5000\n",
    "timesteps = 96\n",
    "num_classes = 5\n",
    "\n",
    "# Generar datos\n",
    "X_ecg = np.random.randn(n_samples, timesteps, 1).astype(np.float32)\n",
    "y_ecg = np.random.randint(0, num_classes, n_samples)\n",
    "\n",
    "# Normalizar\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_ecg.reshape(-1, 1)).reshape(n_samples, timesteps, 1)\n",
    "\n",
    "# Split\n",
    "idx1, idx2 = int(0.7*n_samples), int(0.85*n_samples)\n",
    "X_train, y_train = X_scaled[:idx1], y_ecg[:idx1]\n",
    "X_val, y_val = X_scaled[idx1:idx2], y_ecg[idx1:idx2]\n",
    "X_test, y_test = X_scaled[idx2:], y_ecg[idx2:]\n",
    "\n",
    "# Entrenar\n",
    "model_ecg = build_lstm_model((timesteps, 1), num_classes)\n",
    "start = time.time()\n",
    "history_ecg = model_ecg.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=64, verbose=1)\n",
    "time_ecg = time.time() - start\n",
    "loss_ecg, acc_ecg = model_ecg.evaluate(X_test, y_test, verbose=0)\n",
    "\n",
    "print(f\"\\nAccuracy: {acc_ecg:.4f} | Loss: {loss_ecg:.4f} | Time: {time_ecg:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a06b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entrenar UCI HAR\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TRAINING UCI HAR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "n_train, n_test = 7352, 2947\n",
    "n_features, timesteps_har = 561, 128\n",
    "n_classes_har = 6\n",
    "\n",
    "# Generar datos\n",
    "X_train_har = np.random.randn(n_train, n_features).astype(np.float32)\n",
    "y_train_har = np.random.randint(0, n_classes_har, n_train)\n",
    "X_test_har = np.random.randn(n_test, n_features).astype(np.float32)\n",
    "y_test_har = np.random.randint(0, n_classes_har, n_test)\n",
    "\n",
    "# Normalizar\n",
    "scaler_har = StandardScaler()\n",
    "X_train_har = scaler_har.fit_transform(X_train_har).astype(np.float32)\n",
    "X_test_har = scaler_har.transform(X_test_har).astype(np.float32)\n",
    "\n",
    "# Reshape\n",
    "X_train_lstm = X_train_har[:, :timesteps_har].reshape(-1, timesteps_har, 1)\n",
    "X_test_lstm = X_test_har[:, :timesteps_har].reshape(-1, timesteps_har, 1)\n",
    "\n",
    "# Split\n",
    "idx = int(0.85 * len(X_train_lstm))\n",
    "X_tr, y_tr = X_train_lstm[:idx], y_train_har[:idx]\n",
    "X_v, y_v = X_train_lstm[idx:], y_train_har[idx:]\n",
    "\n",
    "# Entrenar\n",
    "model_har = build_lstm_model((timesteps_har, 1), n_classes_har)\n",
    "start = time.time()\n",
    "history_har = model_har.fit(X_tr, y_tr, validation_data=(X_v, y_v), epochs=30, batch_size=64, verbose=1)\n",
    "time_har = time.time() - start\n",
    "loss_har, acc_har = model_har.evaluate(X_test_lstm, y_test_har, verbose=0)\n",
    "\n",
    "print(f\"\\nAccuracy: {acc_har:.4f} | Loss: {loss_har:.4f} | Time: {time_har:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fd17e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparar resultados\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CPU vs GPU COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "cpu_ecg_time = 55.5\n",
    "cpu_har_time = 543.7\n",
    "\n",
    "print(f\"\\nECG5000:\")\n",
    "print(f\"  CPU: {cpu_ecg_time:.2f}s | GPU: {time_ecg:.2f}s | Speedup: {cpu_ecg_time/time_ecg:.2f}x\")\n",
    "\n",
    "print(f\"\\nUCI HAR:\")\n",
    "print(f\"  CPU: {cpu_har_time:.2f}s | GPU: {time_har:.2f}s | Speedup: {cpu_har_time/time_har:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60b324f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gr√°ficas\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "fig.suptitle('TFM - GPU Training Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "axes[0, 0].plot(history_ecg.history['accuracy'], label='Train')\n",
    "axes[0, 0].plot(history_ecg.history['val_accuracy'], label='Val')\n",
    "axes[0, 0].set_title('ECG5000 - Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].plot(history_ecg.history['loss'], label='Train')\n",
    "axes[0, 1].plot(history_ecg.history['val_loss'], label='Val')\n",
    "axes[0, 1].set_title('ECG5000 - Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].plot(history_har.history['accuracy'], label='Train')\n",
    "axes[1, 0].plot(history_har.history['val_accuracy'], label='Val')\n",
    "axes[1, 0].set_title('UCI HAR - Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(history_har.history['loss'], label='Train')\n",
    "axes[1, 1].plot(history_har.history['val_loss'], label='Val')\n",
    "axes[1, 1].set_title('UCI HAR - Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Gr√°ficas guardadas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1bc14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar resultados\n",
    "results_df = pd.DataFrame({\n",
    "    'Dataset': ['ECG5000', 'UCI_HAR'],\n",
    "    'Accuracy': [acc_ecg, acc_har],\n",
    "    'Loss': [loss_ecg, loss_har],\n",
    "    'Time_s': [time_ecg, time_har]\n",
    "})\n",
    "\n",
    "results_df.to_csv('gpu_results.csv', index=False)\n",
    "print(\"‚úÖ Resultados guardados\")\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a59aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar\n",
    "try:\n",
    "    from google.colab import files\n",
    "    files.download('gpu_results.csv')\n",
    "    files.download('training_results.png')\n",
    "    print(\"‚úÖ Archivos descargados\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è No est√°s en Colab. Archivos creados localmente.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ee92f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"GPU AVAILABILITY CHECK\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "\n",
    "print(f\"\\n‚úì GPUs detectadas: {len(gpu_devices)}\")\n",
    "for gpu in gpu_devices:\n",
    "    print(f\"  - {gpu}\")\n",
    "\n",
    "print(f\"\\n‚úì CPUs detectadas: {len(cpu_devices)}\")\n",
    "for cpu in cpu_devices:\n",
    "    print(f\"  - {cpu}\")\n",
    "\n",
    "if len(gpu_devices) == 0:\n",
    "    print(\"\\n‚ö†Ô∏è  NO GPU DETECTED. Please enable GPU:\")\n",
    "    print(\"   Runtime ‚Üí Change runtime type ‚Üí GPU ‚Üí Save\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ GPU READY FOR TRAINING!\")\n",
    "\n",
    "print(f\"\\nTensorFlow version: {tf.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df42479",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ INSTALAR DEPENDENCIAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58422900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar paquetes necesarios\n",
    "!pip install -q pandas scikit-learn matplotlib numpy pillow\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed\")\n",
    "print(f\"   - TensorFlow {tf.__version__}\")\n",
    "print(f\"   - NumPy {np.__version__}\")\n",
    "print(f\"   - Pandas {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc376133",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ DEFINIR MODELO LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e3748",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "def get_device_info():\n",
    "    \"\"\"Detecta GPU/CPU disponible\"\"\"\n",
    "    gpu_devices = tf.config.list_physical_devices('GPU')\n",
    "    cpu_devices = tf.config.list_physical_devices('CPU')\n",
    "    device_name = \"GPU\" if gpu_devices else \"CPU\"\n",
    "    return {\n",
    "        \"device\": device_name,\n",
    "        \"gpu_count\": len(gpu_devices),\n",
    "        \"cpu_count\": len(cpu_devices)\n",
    "    }\n",
    "\n",
    "def build_lstm_model(input_shape, num_classes, lstm_units=64):\n",
    "    \"\"\"Construye modelo LSTM bidireccional\"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        layers.Bidirectional(layers.LSTM(lstm_units, return_sequences=True)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Bidirectional(layers.LSTM(lstm_units // 2)),\n",
    "        layers.Dropout(0.4),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate_lstm(model, X_train, y_train, X_val, y_val, X_test, y_test, epochs=50):\n",
    "    \"\"\"Entrena y eval√∫a modelo LSTM\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"INICIANDO ENTRENAMIENTO\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Datos de entrenamiento: {X_train.shape}\")\n",
    "    print(f\"Datos de prueba: {X_test.shape}\")\n",
    "    print(f\"√âpocas: {epochs}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=epochs,\n",
    "        batch_size=64,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    end_time = time.time()\n",
    "    training_time = end_time - start_time\n",
    "    \n",
    "    loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Loss: {loss:.4f}\")\n",
    "    print(f\"Tiempo de entrenamiento: {training_time:.2f}s\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    return history, accuracy, loss, training_time\n",
    "\n",
    "print(\"‚úÖ LSTM model functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec7ddad",
   "metadata": {},
   "source": [
    "---\n",
    "## 4Ô∏è‚É£ ENTRENAR CNN - FASHION MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f930131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRENAMIENTO CNN - FASHION MNIST\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar Fashion MNIST\n",
    "from tensorflow.keras import datasets\n",
    "\n",
    "(X_train_fm, y_train_fm), (X_test_fm, y_test_fm) = datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Normalizar\n",
    "X_train_fm = X_train_fm.astype('float32') / 255.0\n",
    "X_test_fm = X_test_fm.astype('float32') / 255.0\n",
    "\n",
    "# Reshape: agregar canal\n",
    "X_train_fm = X_train_fm.reshape(-1, 28, 28, 1)\n",
    "X_test_fm = X_test_fm.reshape(-1, 28, 28, 1)\n",
    "\n",
    "# Split train/val\n",
    "idx_split = int(0.85 * len(X_train_fm))\n",
    "X_val_fm = X_train_fm[idx_split:]\n",
    "y_val_fm = y_train_fm[idx_split:]\n",
    "X_train_fm = X_train_fm[:idx_split]\n",
    "y_train_fm = y_train_fm[:idx_split]\n",
    "\n",
    "print(f\"\\nDatos Fashion MNIST:\")\n",
    "print(f\"  Train: {X_train_fm.shape}\")\n",
    "print(f\"  Val:   {X_val_fm.shape}\")\n",
    "print(f\"  Test:  {X_test_fm.shape}\")\n",
    "\n",
    "# Construir y entrenar modelo CNN\n",
    "def build_cnn_model(input_shape, num_classes):\n",
    "    model = models.Sequential([\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.MaxPooling2D((2, 2)),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_fm = build_cnn_model((28, 28, 1), 10)\n",
    "\n",
    "device_info = get_device_info()\n",
    "print(f\"\\nüéØ Entrenando en: {device_info['device']}\")\n",
    "\n",
    "start = time.time()\n",
    "history_fm = model_fm.fit(X_train_fm, y_train_fm, validation_data=(X_val_fm, y_val_fm), epochs=10, batch_size=128, verbose=1)\n",
    "time_fm = time.time() - start\n",
    "loss_fm, acc_fm = model_fm.evaluate(X_test_fm, y_test_fm, verbose=0)\n",
    "\n",
    "print(f\"\\nAccuracy: {acc_fm:.4f} | Loss: {loss_fm:.4f} | Time: {time_fm:.2f}s\")\n",
    "print(\"‚úÖ Fashion MNIST training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da87a966",
   "metadata": {},
   "source": [
    "---\n",
    "## 5Ô∏è‚É£ ENTRENAR CNN - CIFAR-10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f180a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRENAMIENTO CNN - CIFAR-10\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Cargar CIFAR-10\n",
    "(X_train_c10, y_train_c10), (X_test_c10, y_test_c10) = datasets.cifar10.load_data()\n",
    "y_train_c10 = y_train_c10.flatten()\n",
    "y_test_c10 = y_test_c10.flatten()\n",
    "\n",
    "# Normalizar\n",
    "X_train_c10 = X_train_c10.astype('float32') / 255.0\n",
    "X_test_c10 = X_test_c10.astype('float32') / 255.0\n",
    "\n",
    "# Split train/val\n",
    "idx_split = int(0.85 * len(X_train_c10))\n",
    "X_val_c10 = X_train_c10[idx_split:]\n",
    "y_val_c10 = y_train_c10[idx_split:]\n",
    "X_train_c10 = X_train_c10[:idx_split]\n",
    "y_train_c10 = y_train_c10[:idx_split]\n",
    "\n",
    "print(f\"\\nDatos CIFAR-10:\")\n",
    "print(f\"  Train: {X_train_c10.shape}\")\n",
    "print(f\"  Val:   {X_val_c10.shape}\")\n",
    "print(f\"  Test:  {X_test_c10.shape}\")\n",
    "\n",
    "# Construir y entrenar modelo\n",
    "model_c10 = build_cnn_model((32, 32, 3), 10)\n",
    "\n",
    "print(f\"\\nüéØ Entrenando en: {device_info['device']}\")\n",
    "\n",
    "start = time.time()\n",
    "history_c10 = model_c10.fit(X_train_c10, y_train_c10, validation_data=(X_val_c10, y_val_c10), epochs=10, batch_size=128, verbose=1)\n",
    "time_c10 = time.time() - start\n",
    "loss_c10, acc_c10 = model_c10.evaluate(X_test_c10, y_test_c10, verbose=0)\n",
    "\n",
    "print(f\"\\nAccuracy: {acc_c10:.4f} | Loss: {loss_c10:.4f} | Time: {time_c10:.2f}s\")\n",
    "print(\"‚úÖ CIFAR-10 training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3a7c62",
   "metadata": {},
   "source": [
    "---\n",
    "## 6Ô∏è‚É£ ENTRENAR LSTM - ECG5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f51666",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRENAMIENTO ECG5000\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Generar datos sint√©ticos ECG (similar a ECG5000)\n",
    "np.random.seed(42)\n",
    "n_samples_ecg = 5000\n",
    "timesteps = 96\n",
    "features = 1\n",
    "num_classes_ecg = 5\n",
    "\n",
    "# Generar datos sint√©ticos\n",
    "X_ecg = np.random.randn(n_samples_ecg, timesteps, features).astype(np.float32)\n",
    "y_ecg = np.random.randint(0, num_classes_ecg, n_samples_ecg)\n",
    "\n",
    "# Normalizar\n",
    "scaler = StandardScaler()\n",
    "X_ecg_reshaped = X_ecg.reshape(-1, features)\n",
    "X_ecg_reshaped = scaler.fit_transform(X_ecg_reshaped)\n",
    "X_ecg = X_ecg_reshaped.reshape(n_samples_ecg, timesteps, features)\n",
    "\n",
    "# Split: 70% train, 15% val, 15% test\n",
    "idx_train = int(0.7 * n_samples_ecg)\n",
    "idx_val = int(0.85 * n_samples_ecg)\n",
    "\n",
    "X_train_ecg = X_ecg[:idx_train]\n",
    "y_train_ecg = y_ecg[:idx_train]\n",
    "X_val_ecg = X_ecg[idx_train:idx_val]\n",
    "y_val_ecg = y_ecg[idx_train:idx_val]\n",
    "X_test_ecg = X_ecg[idx_val:]\n",
    "y_test_ecg = y_ecg[idx_val:]\n",
    "\n",
    "print(f\"\\nDatos ECG:\")\n",
    "print(f\"  Train: {X_train_ecg.shape}\")\n",
    "print(f\"  Val:   {X_val_ecg.shape}\")\n",
    "print(f\"  Test:  {X_test_ecg.shape}\")\n",
    "\n",
    "# Construir y entrenar modelo\n",
    "model_ecg = build_lstm_model(\n",
    "    input_shape=(timesteps, features),\n",
    "    num_classes=num_classes_ecg,\n",
    "    lstm_units=64\n",
    ")\n",
    "\n",
    "device_info = get_device_info()\n",
    "print(f\"\\nüéØ Entrenando en: {device_info['device']}\")\n",
    "\n",
    "history_ecg, acc_ecg, loss_ecg, time_ecg = train_and_evaluate_lstm(\n",
    "    model_ecg, X_train_ecg, y_train_ecg, X_val_ecg, y_val_ecg, X_test_ecg, y_test_ecg,\n",
    "    epochs=50\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ ECG5000 training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4602fc15",
   "metadata": {},
   "source": [
    "---\n",
    "## 7Ô∏è‚É£ ENTRENAR LSTM - UCI HAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0151e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ENTRENAMIENTO UCI HAR\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Descargar UCI HAR dataset\n",
    "print(\"\\nDescargando UCI HAR dataset...\")\n",
    "url_har = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI%20HAR%20Dataset.zip\"\n",
    "zip_path = \"/tmp/har.zip\"\n",
    "extract_path = \"/tmp/UCI_HAR\"\n",
    "\n",
    "try:\n",
    "    urllib.request.urlretrieve(url_har, zip_path)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    print(\"‚úÖ Dataset descargado\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Error en descarga: {e}\")\n",
    "    print(\"Usando datos sint√©ticos para UCI HAR...\")\n",
    "\n",
    "# Cargar datos\n",
    "def load_har_data():\n",
    "    try:\n",
    "        X_train = np.loadtxt(f\"{extract_path}/UCI HAR Dataset/train/X_train.txt\")\n",
    "        y_train = np.loadtxt(f\"{extract_path}/UCI HAR Dataset/train/y_train.txt\", dtype=int) - 1\n",
    "        X_test = np.loadtxt(f\"{extract_path}/UCI HAR Dataset/test/X_test.txt\")\n",
    "        y_test = np.loadtxt(f\"{extract_path}/UCI HAR Dataset/test/y_test.txt\", dtype=int) - 1\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    except:\n",
    "        print(\"Generando datos sint√©ticos para HAR...\")\n",
    "        n_train = 7352\n",
    "        n_test = 2947\n",
    "        n_features = 561\n",
    "        n_classes = 6\n",
    "        X_train = np.random.randn(n_train, n_features).astype(np.float32)\n",
    "        y_train = np.random.randint(0, n_classes, n_train)\n",
    "        X_test = np.random.randn(n_test, n_features).astype(np.float32)\n",
    "        y_test = np.random.randint(0, n_classes, n_test)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train_har, y_train_har, X_test_har, y_test_har = load_har_data()\n",
    "\n",
    "# Normalizar\n",
    "scaler_har = StandardScaler()\n",
    "X_train_har = scaler_har.fit_transform(X_train_har).astype(np.float32)\n",
    "X_test_har = scaler_har.transform(X_test_har).astype(np.float32)\n",
    "\n",
    "# Reshape para LSTM (128 timesteps, features reducidas)\n",
    "timesteps_har = 128\n",
    "X_train_har_lstm = X_train_har[:, :timesteps_har].reshape(-1, timesteps_har, 1)\n",
    "X_test_har_lstm = X_test_har[:, :timesteps_har].reshape(-1, timesteps_har, 1)\n",
    "\n",
    "# Split train/val\n",
    "idx_split = int(0.85 * len(X_train_har_lstm))\n",
    "X_val_har = X_train_har_lstm[idx_split:]\n",
    "y_val_har = y_train_har[idx_split:]\n",
    "X_train_har = X_train_har_lstm[:idx_split]\n",
    "y_train_har = y_train_har[:idx_split]\n",
    "\n",
    "print(f\"\\nDatos UCI HAR:\")\n",
    "print(f\"  Train: {X_train_har.shape}\")\n",
    "print(f\"  Val:   {X_val_har.shape}\")\n",
    "print(f\"  Test:  {X_test_har_lstm.shape}\")\n",
    "\n",
    "# Construir y entrenar modelo\n",
    "num_classes_har = 6\n",
    "model_har = build_lstm_model(\n",
    "    input_shape=(timesteps_har, 1),\n",
    "    num_classes=num_classes_har,\n",
    "    lstm_units=64\n",
    ")\n",
    "\n",
    "print(f\"\\nüéØ Entrenando en: {device_info['device']}\")\n",
    "\n",
    "history_har, acc_har, loss_har, time_har = train_and_evaluate_lstm(\n",
    "    model_har, X_train_har, y_train_har, X_val_har, y_val_har, X_test_har_lstm, y_test_har,\n",
    "    epochs=30\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ UCI HAR training completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb2f1b6",
   "metadata": {},
   "source": [
    "---\n",
    "## 8Ô∏è‚É£ COMPARAR RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc67945f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumen de resultados\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESUMEN DE RESULTADOS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nFASE 1 - CNN:\")\n",
    "print(f\"  Fashion MNIST - Accuracy: {acc_fm:.4f}, Loss: {loss_fm:.4f}, Time: {time_fm:.2f}s\")\n",
    "print(f\"  CIFAR-10      - Accuracy: {acc_c10:.4f}, Loss: {loss_c10:.4f}, Time: {time_c10:.2f}s\")\n",
    "\n",
    "print(\"\\nFASE 2 - LSTM:\")\n",
    "print(f\"  ECG5000 - Accuracy: {acc_ecg:.4f}, Loss: {loss_ecg:.4f}, Time: {time_ecg:.2f}s\")\n",
    "print(f\"  UCI HAR - Accuracy: {acc_har:.4f}, Loss: {loss_har:.4f}, Time: {time_har:.2f}s\")\n",
    "\n",
    "print(f\"\\nTiempo total: {(time_fm + time_c10 + time_ecg + time_har):.2f}s\")\n",
    "print(f\"Dispositivo: {device_info['device']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a681b",
   "metadata": {},
   "source": [
    "---\n",
    "## 9Ô∏è‚É£ GENERAR GR√ÅFICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24cdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear gr√°ficas de training\n",
    "fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
    "fig.suptitle('TFM - CNN + LSTM GPU Training Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Row 1: CNN\n",
    "# Fashion MNIST Accuracy\n",
    "axes[0, 0].plot(history_fm.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0, 0].plot(history_fm.history['val_accuracy'], label='Val', linewidth=2)\n",
    "axes[0, 0].set_title('Fashion MNIST - Accuracy', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Fashion MNIST Loss\n",
    "axes[0, 1].plot(history_fm.history['loss'], label='Train', linewidth=2)\n",
    "axes[0, 1].plot(history_fm.history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0, 1].set_title('Fashion MNIST - Loss', fontweight='bold')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Loss')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# CIFAR-10 Accuracy\n",
    "axes[0, 2].plot(history_c10.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[0, 2].plot(history_c10.history['val_accuracy'], label='Val', linewidth=2)\n",
    "axes[0, 2].set_title('CIFAR-10 - Accuracy', fontweight='bold')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Accuracy')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# CIFAR-10 Loss\n",
    "axes[0, 3].plot(history_c10.history['loss'], label='Train', linewidth=2)\n",
    "axes[0, 3].plot(history_c10.history['val_loss'], label='Val', linewidth=2)\n",
    "axes[0, 3].set_title('CIFAR-10 - Loss', fontweight='bold')\n",
    "axes[0, 3].set_xlabel('Epoch')\n",
    "axes[0, 3].set_ylabel('Loss')\n",
    "axes[0, 3].legend()\n",
    "axes[0, 3].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 2: LSTM\n",
    "# ECG5000 Accuracy\n",
    "axes[1, 0].plot(history_ecg.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[1, 0].plot(history_ecg.history['val_accuracy'], label='Val', linewidth=2)\n",
    "axes[1, 0].set_title('ECG5000 - Accuracy', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Accuracy')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# ECG5000 Loss\n",
    "axes[1, 1].plot(history_ecg.history['loss'], label='Train', linewidth=2)\n",
    "axes[1, 1].plot(history_ecg.history['val_loss'], label='Val', linewidth=2)\n",
    "axes[1, 1].set_title('ECG5000 - Loss', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# UCI HAR Accuracy\n",
    "axes[1, 2].plot(history_har.history['accuracy'], label='Train', linewidth=2)\n",
    "axes[1, 2].plot(history_har.history['val_accuracy'], label='Val', linewidth=2)\n",
    "axes[1, 2].set_title('UCI HAR - Accuracy', fontweight='bold')\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('Accuracy')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# UCI HAR Loss\n",
    "axes[1, 3].plot(history_har.history['loss'], label='Train', linewidth=2)\n",
    "axes[1, 3].plot(history_har.history['val_loss'], label='Val', linewidth=2)\n",
    "axes[1, 3].set_title('UCI HAR - Loss', fontweight='bold')\n",
    "axes[1, 3].set_xlabel('Epoch')\n",
    "axes[1, 3].set_ylabel('Loss')\n",
    "axes[1, 3].legend()\n",
    "axes[1, 3].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Gr√°ficas guardadas: training_results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09624633",
   "metadata": {},
   "source": [
    "---\n",
    "## üîü GUARDAR Y DESCARGAR RESULTADOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4975508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear resumen CSV\n",
    "results_df = pd.DataFrame({\n",
    "    'Phase': ['Fase 1 (CNN)', 'Fase 1 (CNN)', 'Fase 2 (LSTM)', 'Fase 2 (LSTM)'],\n",
    "    'Dataset': ['Fashion MNIST', 'CIFAR-10', 'ECG5000', 'UCI HAR'],\n",
    "    'Model': ['CNN', 'CNN', 'LSTM', 'LSTM'],\n",
    "    'Accuracy': [acc_fm, acc_c10, acc_ecg, acc_har],\n",
    "    'Loss': [loss_fm, loss_c10, loss_ecg, loss_har],\n",
    "    'Training_Time_s': [time_fm, time_c10, time_ecg, time_har],\n",
    "    'Device': ['GPU', 'GPU', 'GPU', 'GPU']\n",
    "})\n",
    "\n",
    "results_df.to_csv('gpu_results.csv', index=False)\n",
    "print(\"‚úÖ Resultados guardados: gpu_results.csv\")\n",
    "print(\"\\n\" + results_df.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5253eb6e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• DESCARGAR ARCHIVOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9f704b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descargar archivos desde Colab\n",
    "try:\n",
    "    from google.colab import files\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"DESCARGANDO RESULTADOS\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    files_to_download = ['gpu_results.csv', 'training_results.png']\n",
    "    \n",
    "    for file in files_to_download:\n",
    "        try:\n",
    "            files.download(file)\n",
    "            print(f\"‚úÖ Descargado: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è No se pudo descargar {file}: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"‚úÖ ENTRENAMIENTO COMPLETADO\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\\nResultados guardados en:\")\n",
    "    print(\"  üìä gpu_results.csv\")\n",
    "    print(\"  üìà training_results.png\")\n",
    "except:\n",
    "    print(\"\\n‚ö†Ô∏è No est√°s en Colab. Los archivos se han creado localmente.\")\n",
    "    print(\"   - gpu_results.csv\")\n",
    "    print(\"   - training_results.png\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
